{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from pydub import AudioSegment\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "AudioSegment.ffmpeg = \"C:\\\\installed\\\\ffmpeg\\\\bin\\\\ffmpeg.exe\"\n",
    "AudioSegment.converter = \"C:\\\\installed\\\\ffmpeg\\\\bin\\\\ffmpeg.exe\"\n",
    "\n",
    "from watson_developer_cloud import TextToSpeechV1\n",
    "\n",
    "import glob\n",
    "import json\n",
    "from random import shuffle\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import csv\n",
    "\n",
    "# Rohun's IBM credential\n",
    "tts = TextToSpeechV1(\n",
    "    username='683a066f-a063-4e2e-bcae-1c48690bd515',\n",
    "    password='7oPU2XALqUtv',\n",
    ")\n",
    "def synthesize_number(number):\n",
    "    try:\n",
    "        byte_audio = tts.synthesize(str(number), accept='audio/wav', voice=\"en-US_AllisonVoice\")\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            byte_audio = tts.synthesize(str(number), accept='audio/wav', voice=\"en-US_AllisonVoice\")\n",
    "        except Exception as e:\n",
    "            return AudioSegment.silent(duration=250)\n",
    "\n",
    "    # The sample_width, frame_rate and the channels are empirical values. No values in documentation.\n",
    "    sound = AudioSegment(\n",
    "        data=byte_audio,\n",
    "\n",
    "        sample_width=2,\n",
    "        frame_rate=22050,\n",
    "        channels=1\n",
    "    )\n",
    "\n",
    "    return sound\n",
    "\n",
    "\n",
    "def stich_audio_files(file_list, silence, post_number_silence, location, tag_extension):\n",
    "    stiched_result = AudioSegment.empty()\n",
    "\n",
    "    USER_STUDY_OUTPUT_DATA = \"user_study_output\\\\\"\n",
    "    for index, audio_file in enumerate(file_list):\n",
    "        try:\n",
    "            audio = AudioSegment.from_file(USER_STUDY_OUTPUT_DATA + location + audio_file + tag_extension, format=\"wav\")\n",
    "            number = synthesize_number(index + 1)\n",
    "            stiched_result += silence + number + post_number_silence + audio\n",
    "        except Exception as e:\n",
    "            logging.error(str(e))\n",
    "\n",
    "    return stiched_result\n",
    "\n",
    "\n",
    "\n",
    "def crop_and_save_to_wav(file_name):\n",
    "    just_name = file_name.rfind(\".\")\n",
    "    audio = AudioSegment.from_file(file_name, format=\"mp3\")[3000:6000]\n",
    "    \n",
    "    for value in [0, 10, 20, 30, 40]:\n",
    "        reduced_audio = audio - value\n",
    "        reduced_audio.export(file_name[:just_name] + \"_\" + str(value) + \".wav\", format=\"wav\")\n",
    "\n",
    "\n",
    "        \n",
    "def decrease_beep_strength():\n",
    "    beep = AudioSegment.from_file(\"data_input\\\\\" + 'beep.wav', 'wav')\n",
    "    for i in range(5):\n",
    "        beep = beep - 10\n",
    "        beep.export(out_f=\"data_input\\\\beep_\" + str(i) + \".wav\", format=\"wav\")\n",
    "        \n",
    "\n",
    "    \n",
    "'Reads files with the specified Noise types in allowed values'\n",
    "\n",
    "def read_files(file_list, allowed_values, columns):\n",
    "    result_dataframe = pd.DataFrame()\n",
    "    \n",
    "    for file_name in file_list:\n",
    "        data_frame = pd.read_csv(file_name, names=columns, usecols=range(len(columns)))\n",
    "        if len(data_frame) == 0:\n",
    "            continue\n",
    "        \n",
    "        result_dataframe = result_dataframe.append(data_frame, ignore_index=True)\n",
    "    \n",
    "    result_dataframe = result_dataframe[result_dataframe[\"noise_type\"].isin(allowed_values)]\n",
    "    \n",
    "    result_dataframe = result_dataframe[result_dataframe[\"noise\"] != -1]\n",
    "    \n",
    "    return result_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'audio': 'output_149449394454942.wav', 'gt': '8901680168'}, {'audio': 'output_149449657320933.wav', 'gt': '4341785780'}, {'audio': 'output_1494487771961015.wav', 'gt': '8133099358'}, {'audio': 'output_1494493574786677.wav', 'gt': '6238408884'}, {'audio': 'output_1494496272313691.wav', 'gt': '7359247232'}, {'audio': 'output_1494496294872982.wav', 'gt': '1010593916'}, {'audio': 'output_1494496519281245.wav', 'gt': '3869524001'}]\n"
     ]
    }
   ],
   "source": [
    "def stich_audio_alternate_files():\n",
    "    \"\"\"Stiches 5 second random char Google CAPTCHAs to create 10 second CAPTCHAs\"\"\"\n",
    "    \n",
    "    items = json.loads('''[ {\"audio\":\"output_149449394454942.wav\", \"gt\":\"89016\"},\n",
    "  {\"audio\":\"output_149449657320933.wav\", \"gt\":\"43417\"},\n",
    "  {\"audio\":\"output_1494487771961015.wav\", \"gt\":\"81330\"},\n",
    "  {\"audio\":\"output_1494493574786677.wav\", \"gt\":\"62384\"},\n",
    "  {\"audio\":\"output_1494496272313691.wav\", \"gt\":\"73592\"},\n",
    "  {\"audio\":\"output_1494496294872982.wav\", \"gt\":\"10105\"},\n",
    "  {\"audio\":\"output_1494496519281245.wav\", \"gt\":\"38695\"},\n",
    "  {\"audio\":\"output_1494496601054923.wav\", \"gt\":\"80168\"},\n",
    "  {\"audio\":\"output_1494496664266538.wav\", \"gt\":\"85780\"},\n",
    "  {\"audio\":\"output_1494497754558948.wav\", \"gt\":\"99358\"},\n",
    "  {\"audio\":\"output_1494497804519805.wav\", \"gt\":\"08884\"},\n",
    "  {\"audio\":\"output_1494497859133929.wav\", \"gt\":\"47232\"},\n",
    "  {\"audio\":\"output_1494497882023238.wav\", \"gt\":\"93916\"},\n",
    "  {\"audio\":\"output_1494497934197222.wav\", \"gt\":\"24001\"}]''') \n",
    "    \n",
    "    list1 = []\n",
    "    \n",
    "    for first, second in zip(items[0:7], items[7:]):    \n",
    "        stiched_result = AudioSegment.empty()\n",
    "\n",
    "        c1_source_data = \"Test_Data\\\\v1\\\\c1\\\\Google_Captcha_Demo_5_CHAR\\\\\"\n",
    "        \n",
    "        c1_output_data = \"Test_Data\\\\v3_2\\\\c1\\\\\"\n",
    "        \n",
    "        audio = AudioSegment.from_file(c1_source_data + first[\"audio\"], format=\"wav\")\n",
    "        stiched_result += audio\n",
    "        audio = AudioSegment.from_file(c1_source_data + second[\"audio\"], format=\"wav\")\n",
    "        stiched_result += audio\n",
    "        \n",
    "        gt = first[\"gt\"] + second[\"gt\"]\n",
    "        \n",
    "        list1.append({\"audio\" : first[\"audio\"], \"gt\" : gt})\n",
    "        \n",
    "        \n",
    "        # stiched_result.export(c1_output_data + first[\"audio\"], \"wav\")\n",
    "    print(str(list1))\n",
    "\n",
    "stich_audio_alternate_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Parameters that are being used in this exercise'''\n",
    "\n",
    "# Data Set Iteration number\n",
    "data_version = \"3b\"\n",
    "\n",
    "# Captcha Version. Options - 2, 3a, 3b, 4\n",
    "captcha_type = \"4\"\n",
    "\n",
    "# filter_keyword_list = [\"REFACTORED_White_YT_VERSION_\" + captcha_type, \"REFACTORED_White_PODCAST_VERSION_\" + captcha_type, \"REFACTORED_PODCAST_VERSION_\" + captcha_type]\n",
    "filter_keyword_list = [\"REDONE_REFACTORED_PODCAST_VERSION_\" + captcha_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add tag and time to name and use json to dump file names. json file uses same name.\n",
    "\n",
    "complete_file_list = []\n",
    "for filter_keyword in filter_keyword_list:\n",
    "    complete_file_list.extend(glob.glob(\"logs\\\\*selected_*\" + filter_keyword + \".csv\"))\n",
    "\n",
    "if captcha_type == \"4\":\n",
    "    column_word = [\"name\", \"version\", \"original_text\", \"noise\", \"complete\", \"source_type\", \"noise_type\", \"transcript\", \"reduced_word\"]\n",
    "else:\n",
    "    column_word = [\"name\", \"version\", \"start\", \"end\", \"original_text\", \"noise\", \"first_word_easy\", \"first_confidence\", \"second_confidence\", \"source_type\", \"noise_type\", \"first_word\", \"second_word\"]\n",
    "\n",
    "file_dataframe= read_files(complete_file_list, [\"White\"], column_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_filtered_aname_file(file_text, data_version, captcha_type):\n",
    "    'Copies the N files selected and stored in the log file from the specified source location to \"vX\" destination folder'\n",
    "        \n",
    "\n",
    "    audio_name_list = []\n",
    "   \n",
    "    for audio_file in file_text.split(\"\\n\"):\n",
    "        audio_name_list.append({\"audio\" : audio_file.strip()})\n",
    "\n",
    "    audio_folder = \"Test_data\\\\v\" + data_version + \"\\\\audioname\"\n",
    "    os.makedirs(audio_folder, exist_ok = True)\n",
    "    json.dump(audio_name_list, open(os.path.join(audio_folder, \"aname\" + captcha_type + \".json\"), \"w\"))\n",
    "    \n",
    "    \n",
    "create_filtered_aname_file('''Mcluhan-Mckenna_1_chunk_27_1499080254532628_count_1_noise_26_noise_type_White.wav\n",
    "Shakespeare_chunk_60_1497156637776667_count_0_noise_10_noise_type_White.wav\n",
    "TerenceMckenna-TrueHallucinations09-16_chunk_64_1499080254532628_count_0_noise_7_noise_type_White.wav\n",
    "American_Civil_War_chunk_54_1497156637776667_count_0_noise_28_noise_type_White.wav\n",
    "American_revolution_lecture_chunk_47_1497156637776667_count_3_noise_15_noise_type_White.wav\n",
    "Darwins_Legacy_chunk_151_1497156637776667_count_0_noise_27_noise_type_White.wav\n",
    "History_4A_Fall_2007_UC_Berkeley_Lecture_24_Monarchy_at_Rome_The_Age_of_Augustus_20476_chunk_111_1499080254532628_count_0_noise_0_noise_type_White.wav\n",
    "History_4A_Fall_2007_UC_Berkeley_Lecture_24_Monarchy_at_Rome_The_Age_of_Augustus_20476_chunk_129_1499080254532628_count_0_noise_0_noise_type_White.wav\n",
    "Nikola_Tesla_chunk_30_1497156637776667_count_1_noise_7_noise_type_White.wav\n",
    "Shakespeare_chunk_38_1497156637776667_count_0_noise_23_noise_type_White.wav\n",
    "TerenceMckenna-TrueHallucinations08-16_chunk_21_1499080254532628_count_0_noise_0_noise_type_White.wav\n",
    "TerenceMckenna-TrueHallucinations08-16_chunk_82_1499080254532628_count_0_noise_6_noise_type_White.wav\n",
    "TerenceMckenna-TrueHallucinations12-16_chunk_64_1499080254532628_count_0_noise_18_noise_type_White.wav\n",
    "Yoshua_Bengio_chunk_59_1497156637776667_count_0_noise_4_noise_type_White.wav\n",
    "Google_IO_2017_chunk_32_1497156637776667_count_0_noise_23_noise_type_White.wav\n",
    "Nikola_Tesla_chunk_15_1497534345207332_count_0_noise_22_noise_type_White.wav\n",
    "TerenceMckenna-TrueHallucinations02-16_chunk_58_1499080254532628_count_0_noise_14_noise_type_White.wav\n",
    "TerenceMckenna-TrueHallucinations06-16_chunk_97_1499080254532628_count_0_noise_13_noise_type_White.wav\n",
    "TerenceMckenna-TrueHallucinations08-16_chunk_3_1499080254532628_count_1_noise_17_noise_type_White.wav\n",
    "TerenceMckenna-TrueHallucinations08-16_chunk_41_1499080254532628_count_0_noise_9_noise_type_White.wav\n",
    "TheVoynichManuscript_chunk_117_1499080254532628_count_1_noise_0_noise_type_White.wav\n",
    "Western_Philosophy_chunk_58_1497534345207332_count_0_noise_15_noise_type_White.wav\n",
    "Yoshua_Bengio_chunk_59_1497156637776667_count_1_noise_0_noise_type_White.wav\n",
    "American_revolution_lecture_chunk_107_1497534345207332_count_2_noise_23_noise_type_White.wav\n",
    "TerenceMckenna-TrueHallucinations04-16_chunk_57_1499080254532628_count_0_noise_25_noise_type_White.wav\n",
    "TerenceMckenna-TrueHallucinations08-16_chunk_82_1499080254532628_count_1_noise_26_noise_type_White.wav\n",
    "TerenceMckenna-TrueHallucinations09-16_chunk_63_1499080254532628_count_0_noise_11_noise_type_White.wav\n",
    "TerenceMckenna-TrueHallucinations10-16_chunk_14_1499080254532628_count_1_noise_19_noise_type_White.wav\n",
    "FinneginsWake_chunk_129_1499080254532628_count_0_noise_7_noise_type_White.wav\n",
    "google_Documentary_chunk_60_1497156637776667_count_0_noise_9_noise_type_White.wav\n",
    "Google_IO_2017_chunk_196_1497534345207332_count_1_noise_19_noise_type_White.wav\n",
    "Google_IO_2017_chunk_96_1497156637776667_count_0_noise_18_noise_type_White.wav\n",
    "TerenceMckenna-TrueHallucinations06-16_chunk_63_1499080254532628_count_0_noise_21_noise_type_White.wav\n",
    "TerenceMckenna-TrueHallucinations10-16_chunk_14_1499080254532628_count_0_noise_13_noise_type_White.wav\n",
    "TerenceMckenna-TrueHallucinations12-16_chunk_63_1499080254532628_count_0_noise_29_noise_type_White.wav\n",
    "TerenceMckenna-TrueHallucinations15-16_chunk_46_1499080254532628_count_0_noise_31_noise_type_White.wav''', \"3_3\", \"4\")    \n",
    "    \n",
    "def create_ten(file_dataframe, source, data_version, captcha_type):\n",
    "    'Copies the N files selected and stored in the log file from the specified source location to \"vX\" destination folder'\n",
    "    \n",
    "    NUMBER_TO_SELECT = 1000\n",
    "    \n",
    "    destination = \"Test_data\\\\v\" + data_version + \"\\\\c\" + captcha_type + \"_\" + str(datetime.now().timestamp()).replace(\".\",\"\")\n",
    "    os.makedirs(destination, exist_ok = True)\n",
    "\n",
    "    dictionary_gt = []\n",
    "    audio_name_list = []\n",
    "    rows = [[\"Name\", \"Captcha\", \"Word_Detected\", \"Text_Detected\"]]\n",
    "   \n",
    "    for index, row in file_dataframe.head(NUMBER_TO_SELECT).iterrows():\n",
    "        audio_file = row[\"name\"]    \n",
    "        \n",
    "        if captcha_type == \"4\":\n",
    "            transcript = row[\"reduced_word\"]\n",
    "        else: \n",
    "            if row[\"first_word_easy\"]:\n",
    "                transcript = row[\"first_word\"]\n",
    "            else:\n",
    "                transcript = row[\"second_word\"]\n",
    "        \n",
    "        dictionary_gt.append({\"audio\" : audio_file + \".wav\", \"gt\" : transcript})\n",
    "        audio_name_list.append({\"audio\" : audio_file + \".wav\"})\n",
    "        rows.append([audio_file, captcha_type])\n",
    "        \n",
    "        audio = AudioSegment.from_file(source + audio_file + \".wav\", format=\"wav\")\n",
    "        output_path = os.path.join(destination, audio_file + \".wav\")\n",
    "        audio.export(output_path, format=\"wav\")\n",
    "        \n",
    "    gt_folder = \"Test_data\\\\v\" + data_version + \"\\\\gt_data\"\n",
    "    os.makedirs(gt_folder, exist_ok = True)\n",
    "    json.dump(dictionary_gt, open(os.path.join(gt_folder, \"gt\" + captcha_type + \".json\"), \"w\"))\n",
    "    \n",
    "    audio_folder = \"Test_data\\\\v\" + data_version + \"\\\\audioname\"\n",
    "    os.makedirs(audio_folder, exist_ok = True)\n",
    "    json.dump(audio_name_list, open(os.path.join(audio_folder, \"aname\" + captcha_type + \".json\"), \"w\"))\n",
    "    \n",
    "    selection_folder = \"Test_data\\\\v\" + data_version + \"\\\\selection\"\n",
    "    os.makedirs(selection_folder, exist_ok = True)\n",
    "    with open(os.path.join(selection_folder, \"sample\" + captcha_type + \".csv\"), \"w\", newline=\"\") as sample_file:\n",
    "        csv.writer(sample_file).writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>version</th>\n",
       "      <th>original_text</th>\n",
       "      <th>noise</th>\n",
       "      <th>complete</th>\n",
       "      <th>source_type</th>\n",
       "      <th>noise_type</th>\n",
       "      <th>transcript</th>\n",
       "      <th>reduced_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>History_4A_Fall_2007_UC_Berkeley_Lecture_27_St...</td>\n",
       "      <td>4</td>\n",
       "      <td>perimeter was reinforced by these artificial d...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'length': 9, 'confidence': 0.7673, 'end_time'...</td>\n",
       "      <td>podcast_lecture</td>\n",
       "      <td>White</td>\n",
       "      <td>[{'timestamps': [['for', 0.32, 0.43], ['him', ...</td>\n",
       "      <td>perimeter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TerenceMckenna-TrueHallucinations07-16_chunk_5...</td>\n",
       "      <td>4</td>\n",
       "      <td>approaching break through it says to me</td>\n",
       "      <td>18</td>\n",
       "      <td>{'length': 11, 'confidence': 0.9998, 'end_time...</td>\n",
       "      <td>podcast_lecture</td>\n",
       "      <td>White</td>\n",
       "      <td>[{'timestamps': [['approaching', 0.32, 0.89], ...</td>\n",
       "      <td>through</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>OpenDoors_chunk_131_1499227381638939_count_0_n...</td>\n",
       "      <td>4</td>\n",
       "      <td>transforming machine now that seem to do</td>\n",
       "      <td>12</td>\n",
       "      <td>{'length': 12, 'confidence': 0.9998, 'end_time...</td>\n",
       "      <td>podcast_lecture</td>\n",
       "      <td>White</td>\n",
       "      <td>[{'timestamps': [['transforming', 0.28, 1.22],...</td>\n",
       "      <td>that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>DennisMcKenna5_chunk_19_1499227381638939_count...</td>\n",
       "      <td>4</td>\n",
       "      <td>only held off lady that I signed</td>\n",
       "      <td>0</td>\n",
       "      <td>{'length': 4, 'confidence': 0.3752, 'end_time'...</td>\n",
       "      <td>podcast_lecture</td>\n",
       "      <td>White</td>\n",
       "      <td>[{'timestamps': [['I', 0.23, 0.27], ['only', 0...</td>\n",
       "      <td>signed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>History_4A_Fall_2007_UC_Berkeley_Lecture_28_Cr...</td>\n",
       "      <td>4</td>\n",
       "      <td>was captured by the fast king shop</td>\n",
       "      <td>8</td>\n",
       "      <td>{'length': 3, 'confidence': 0.9844, 'end_time'...</td>\n",
       "      <td>podcast_lecture</td>\n",
       "      <td>White</td>\n",
       "      <td>[{'timestamps': [['was', 0.29, 0.41], ['captur...</td>\n",
       "      <td>king</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>FinneginsWake_chunk_168_1499227381638939_count...</td>\n",
       "      <td>4</td>\n",
       "      <td>not our apartments then we're no better</td>\n",
       "      <td>29</td>\n",
       "      <td>{'length': 3, 'confidence': 0.4126, 'end_time'...</td>\n",
       "      <td>podcast_lecture</td>\n",
       "      <td>White</td>\n",
       "      <td>[{'timestamps': [['our', 0.3, 0.54], ['apartme...</td>\n",
       "      <td>then</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>BeyondPsychology_chunk_91_1499259869456154_cou...</td>\n",
       "      <td>4</td>\n",
       "      <td>a visible glow and the next year</td>\n",
       "      <td>26</td>\n",
       "      <td>{'confidence': 0.6442, 'start_time': 1.8, 'wor...</td>\n",
       "      <td>podcast_lecture</td>\n",
       "      <td>White</td>\n",
       "      <td>[{'transcript': 'video ', 'word_confidence': [...</td>\n",
       "      <td>next</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>FinneginsWake_chunk_127_1499259869456154_count...</td>\n",
       "      <td>4</td>\n",
       "      <td>tension within her cares as it says</td>\n",
       "      <td>19</td>\n",
       "      <td>{'confidence': 0.5761, 'start_time': 2.59, 'wo...</td>\n",
       "      <td>podcast_lecture</td>\n",
       "      <td>White</td>\n",
       "      <td>[{'transcript': 'tension offend her cares as i...</td>\n",
       "      <td>cares</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>History_4A_Fall_2007_UC_Berkeley_Lecture_04_Hi...</td>\n",
       "      <td>4</td>\n",
       "      <td>a time drought but that route was</td>\n",
       "      <td>9</td>\n",
       "      <td>{'confidence': 0.9435, 'start_time': 17.65, 'w...</td>\n",
       "      <td>podcast_lecture</td>\n",
       "      <td>White</td>\n",
       "      <td>[{'transcript': 'this time around but that tha...</td>\n",
       "      <td>drought</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>History_4A_Fall_2007_UC_Berkeley_Lecture_04_Hi...</td>\n",
       "      <td>4</td>\n",
       "      <td>that route was when you could harvest</td>\n",
       "      <td>9</td>\n",
       "      <td>{'confidence': 0.8613, 'start_time': 19.89, 'w...</td>\n",
       "      <td>podcast_lecture</td>\n",
       "      <td>White</td>\n",
       "      <td>[{'transcript': 'that that was when you could ...</td>\n",
       "      <td>harvest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>History_4A_Fall_2007_UC_Berkeley_Lecture_29_Th...</td>\n",
       "      <td>4</td>\n",
       "      <td>that argument of sleep is because I</td>\n",
       "      <td>14</td>\n",
       "      <td>{'confidence': 0.9633, 'start_time': 5.48, 'wo...</td>\n",
       "      <td>podcast_lecture</td>\n",
       "      <td>White</td>\n",
       "      <td>[{'transcript': 'arguing the fleet is because ...</td>\n",
       "      <td>that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>episode_20080602_132430-0700_chunk_139_1499259...</td>\n",
       "      <td>4</td>\n",
       "      <td>in my worst state sellers back I</td>\n",
       "      <td>0</td>\n",
       "      <td>{'confidence': 0.9547, 'start_time': 12.74, 'w...</td>\n",
       "      <td>podcast_lecture</td>\n",
       "      <td>White</td>\n",
       "      <td>[{'transcript': 'my worst sellers back ', 'wor...</td>\n",
       "      <td>back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>TerenceMckenna-TrueHallucinations01-16_chunk_3...</td>\n",
       "      <td>4</td>\n",
       "      <td>this interesting how Senate action interested in</td>\n",
       "      <td>8</td>\n",
       "      <td>{'confidence': 0.6897, 'start_time': 6.78, 'wo...</td>\n",
       "      <td>podcast_lecture</td>\n",
       "      <td>White</td>\n",
       "      <td>[{'transcript': 'it is interesting how the Sen...</td>\n",
       "      <td>interested</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>TerenceMckenna-TrueHallucinations01-16_chunk_3...</td>\n",
       "      <td>4</td>\n",
       "      <td>newly discovered how to send does not</td>\n",
       "      <td>15</td>\n",
       "      <td>{'confidence': 0.9886, 'start_time': 10.28, 'w...</td>\n",
       "      <td>podcast_lecture</td>\n",
       "      <td>White</td>\n",
       "      <td>[{'transcript': 'newly discovered however sent...</td>\n",
       "      <td>does</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>TerenceMckenna-TrueHallucinations15-16_chunk_4...</td>\n",
       "      <td>4</td>\n",
       "      <td>heard before that contact was more than</td>\n",
       "      <td>31</td>\n",
       "      <td>{'confidence': 0.454, 'start_time': 14.28, 'wo...</td>\n",
       "      <td>podcast_lecture</td>\n",
       "      <td>White</td>\n",
       "      <td>[{'transcript': 'heard before the contact with...</td>\n",
       "      <td>more</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>UnfoldingTheLeaf_chunk_6_1499259869456154_coun...</td>\n",
       "      <td>4</td>\n",
       "      <td>things we learned from women Judy green</td>\n",
       "      <td>6</td>\n",
       "      <td>{'confidence': 0.9999, 'start_time': 16.59, 'w...</td>\n",
       "      <td>podcast_lecture</td>\n",
       "      <td>White</td>\n",
       "      <td>[{'transcript': 'things we learned from linen ...</td>\n",
       "      <td>green</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>History_4A_Fall_2007_UC_Berkeley_Lecture_31_Tw...</td>\n",
       "      <td>4</td>\n",
       "      <td>get a foreign province to milk the</td>\n",
       "      <td>2</td>\n",
       "      <td>{'confidence': 0.932, 'start_time': 10.07, 'wo...</td>\n",
       "      <td>podcast_lecture</td>\n",
       "      <td>White</td>\n",
       "      <td>[{'transcript': 'yeah foreign products milk ',...</td>\n",
       "      <td>milk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>TheVoynichManuscript_chunk_116_149925986945615...</td>\n",
       "      <td>4</td>\n",
       "      <td>that time decisive it either not be</td>\n",
       "      <td>23</td>\n",
       "      <td>{'confidence': 0.9975, 'start_time': 21.45, 'w...</td>\n",
       "      <td>podcast_lecture</td>\n",
       "      <td>White</td>\n",
       "      <td>[{'transcript': 'that time I thought that I mi...</td>\n",
       "      <td>that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>History_4A_Fall_2007_UC_Berkeley_Lecture_22_Vi...</td>\n",
       "      <td>4</td>\n",
       "      <td>in Rome and is running around with</td>\n",
       "      <td>8</td>\n",
       "      <td>{'confidence': 0.8416, 'start_time': 10.61, 'w...</td>\n",
       "      <td>podcast_lecture</td>\n",
       "      <td>White</td>\n",
       "      <td>[{'transcript': 'around and is running around ...</td>\n",
       "      <td>with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>TerenceMckenna-TrueHallucinations12-16_chunk_1...</td>\n",
       "      <td>4</td>\n",
       "      <td>does get a French reversal is severing</td>\n",
       "      <td>5</td>\n",
       "      <td>{'confidence': 0.9005, 'start_time': 20.34, 'w...</td>\n",
       "      <td>podcast_lecture</td>\n",
       "      <td>White</td>\n",
       "      <td>[{'transcript': 'does get a French reversal if...</td>\n",
       "      <td>reversal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Mcluhan-Mckenna_1_chunk_59_1499259869456154_co...</td>\n",
       "      <td>4</td>\n",
       "      <td>on earth name dented printing and yet</td>\n",
       "      <td>22</td>\n",
       "      <td>{'confidence': 0.8172, 'start_time': 11.17, 'w...</td>\n",
       "      <td>podcast_lecture</td>\n",
       "      <td>White</td>\n",
       "      <td>[{'transcript': 'for her fame invented printin...</td>\n",
       "      <td>printing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>TerenceMckenna-TrueHallucinations06-16_chunk_9...</td>\n",
       "      <td>4</td>\n",
       "      <td>feel them if they truly believe themselves</td>\n",
       "      <td>13</td>\n",
       "      <td>{'confidence': 0.4131, 'start_time': 0.73, 'wo...</td>\n",
       "      <td>podcast_lecture</td>\n",
       "      <td>White</td>\n",
       "      <td>[{'transcript': 'he will come if they truly be...</td>\n",
       "      <td>themselves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>TheVoynichManuscript_chunk_47_1499259869456154...</td>\n",
       "      <td>4</td>\n",
       "      <td>like Robert flood who was essentially the</td>\n",
       "      <td>9</td>\n",
       "      <td>{'confidence': 0.9599, 'start_time': 14.03, 'w...</td>\n",
       "      <td>podcast_lecture</td>\n",
       "      <td>White</td>\n",
       "      <td>[{'transcript': 'like Robert flood who was ess...</td>\n",
       "      <td>flood</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 name  version  \\\n",
       "2   History_4A_Fall_2007_UC_Berkeley_Lecture_27_St...        4   \n",
       "6   TerenceMckenna-TrueHallucinations07-16_chunk_5...        4   \n",
       "7   OpenDoors_chunk_131_1499227381638939_count_0_n...        4   \n",
       "10  DennisMcKenna5_chunk_19_1499227381638939_count...        4   \n",
       "15  History_4A_Fall_2007_UC_Berkeley_Lecture_28_Cr...        4   \n",
       "19  FinneginsWake_chunk_168_1499227381638939_count...        4   \n",
       "20  BeyondPsychology_chunk_91_1499259869456154_cou...        4   \n",
       "22  FinneginsWake_chunk_127_1499259869456154_count...        4   \n",
       "23  History_4A_Fall_2007_UC_Berkeley_Lecture_04_Hi...        4   \n",
       "24  History_4A_Fall_2007_UC_Berkeley_Lecture_04_Hi...        4   \n",
       "26  History_4A_Fall_2007_UC_Berkeley_Lecture_29_Th...        4   \n",
       "28  episode_20080602_132430-0700_chunk_139_1499259...        4   \n",
       "29  TerenceMckenna-TrueHallucinations01-16_chunk_3...        4   \n",
       "30  TerenceMckenna-TrueHallucinations01-16_chunk_3...        4   \n",
       "32  TerenceMckenna-TrueHallucinations15-16_chunk_4...        4   \n",
       "34  UnfoldingTheLeaf_chunk_6_1499259869456154_coun...        4   \n",
       "35  History_4A_Fall_2007_UC_Berkeley_Lecture_31_Tw...        4   \n",
       "38  TheVoynichManuscript_chunk_116_149925986945615...        4   \n",
       "43  History_4A_Fall_2007_UC_Berkeley_Lecture_22_Vi...        4   \n",
       "47  TerenceMckenna-TrueHallucinations12-16_chunk_1...        4   \n",
       "48  Mcluhan-Mckenna_1_chunk_59_1499259869456154_co...        4   \n",
       "50  TerenceMckenna-TrueHallucinations06-16_chunk_9...        4   \n",
       "51  TheVoynichManuscript_chunk_47_1499259869456154...        4   \n",
       "\n",
       "                                        original_text  noise  \\\n",
       "2   perimeter was reinforced by these artificial d...      0   \n",
       "6             approaching break through it says to me     18   \n",
       "7            transforming machine now that seem to do     12   \n",
       "10                   only held off lady that I signed      0   \n",
       "15                 was captured by the fast king shop      8   \n",
       "19            not our apartments then we're no better     29   \n",
       "20                   a visible glow and the next year     26   \n",
       "22                tension within her cares as it says     19   \n",
       "23                  a time drought but that route was      9   \n",
       "24              that route was when you could harvest      9   \n",
       "26                that argument of sleep is because I     14   \n",
       "28                   in my worst state sellers back I      0   \n",
       "29   this interesting how Senate action interested in      8   \n",
       "30              newly discovered how to send does not     15   \n",
       "32            heard before that contact was more than     31   \n",
       "34            things we learned from women Judy green      6   \n",
       "35                 get a foreign province to milk the      2   \n",
       "38                that time decisive it either not be     23   \n",
       "43                 in Rome and is running around with      8   \n",
       "47             does get a French reversal is severing      5   \n",
       "48              on earth name dented printing and yet     22   \n",
       "50         feel them if they truly believe themselves     13   \n",
       "51          like Robert flood who was essentially the      9   \n",
       "\n",
       "                                             complete      source_type  \\\n",
       "2   {'length': 9, 'confidence': 0.7673, 'end_time'...  podcast_lecture   \n",
       "6   {'length': 11, 'confidence': 0.9998, 'end_time...  podcast_lecture   \n",
       "7   {'length': 12, 'confidence': 0.9998, 'end_time...  podcast_lecture   \n",
       "10  {'length': 4, 'confidence': 0.3752, 'end_time'...  podcast_lecture   \n",
       "15  {'length': 3, 'confidence': 0.9844, 'end_time'...  podcast_lecture   \n",
       "19  {'length': 3, 'confidence': 0.4126, 'end_time'...  podcast_lecture   \n",
       "20  {'confidence': 0.6442, 'start_time': 1.8, 'wor...  podcast_lecture   \n",
       "22  {'confidence': 0.5761, 'start_time': 2.59, 'wo...  podcast_lecture   \n",
       "23  {'confidence': 0.9435, 'start_time': 17.65, 'w...  podcast_lecture   \n",
       "24  {'confidence': 0.8613, 'start_time': 19.89, 'w...  podcast_lecture   \n",
       "26  {'confidence': 0.9633, 'start_time': 5.48, 'wo...  podcast_lecture   \n",
       "28  {'confidence': 0.9547, 'start_time': 12.74, 'w...  podcast_lecture   \n",
       "29  {'confidence': 0.6897, 'start_time': 6.78, 'wo...  podcast_lecture   \n",
       "30  {'confidence': 0.9886, 'start_time': 10.28, 'w...  podcast_lecture   \n",
       "32  {'confidence': 0.454, 'start_time': 14.28, 'wo...  podcast_lecture   \n",
       "34  {'confidence': 0.9999, 'start_time': 16.59, 'w...  podcast_lecture   \n",
       "35  {'confidence': 0.932, 'start_time': 10.07, 'wo...  podcast_lecture   \n",
       "38  {'confidence': 0.9975, 'start_time': 21.45, 'w...  podcast_lecture   \n",
       "43  {'confidence': 0.8416, 'start_time': 10.61, 'w...  podcast_lecture   \n",
       "47  {'confidence': 0.9005, 'start_time': 20.34, 'w...  podcast_lecture   \n",
       "48  {'confidence': 0.8172, 'start_time': 11.17, 'w...  podcast_lecture   \n",
       "50  {'confidence': 0.4131, 'start_time': 0.73, 'wo...  podcast_lecture   \n",
       "51  {'confidence': 0.9599, 'start_time': 14.03, 'w...  podcast_lecture   \n",
       "\n",
       "   noise_type                                         transcript reduced_word  \n",
       "2       White  [{'timestamps': [['for', 0.32, 0.43], ['him', ...    perimeter  \n",
       "6       White  [{'timestamps': [['approaching', 0.32, 0.89], ...      through  \n",
       "7       White  [{'timestamps': [['transforming', 0.28, 1.22],...         that  \n",
       "10      White  [{'timestamps': [['I', 0.23, 0.27], ['only', 0...       signed  \n",
       "15      White  [{'timestamps': [['was', 0.29, 0.41], ['captur...         king  \n",
       "19      White  [{'timestamps': [['our', 0.3, 0.54], ['apartme...         then  \n",
       "20      White  [{'transcript': 'video ', 'word_confidence': [...         next  \n",
       "22      White  [{'transcript': 'tension offend her cares as i...        cares  \n",
       "23      White  [{'transcript': 'this time around but that tha...      drought  \n",
       "24      White  [{'transcript': 'that that was when you could ...      harvest  \n",
       "26      White  [{'transcript': 'arguing the fleet is because ...         that  \n",
       "28      White  [{'transcript': 'my worst sellers back ', 'wor...         back  \n",
       "29      White  [{'transcript': 'it is interesting how the Sen...   interested  \n",
       "30      White  [{'transcript': 'newly discovered however sent...         does  \n",
       "32      White  [{'transcript': 'heard before the contact with...         more  \n",
       "34      White  [{'transcript': 'things we learned from linen ...        green  \n",
       "35      White  [{'transcript': 'yeah foreign products milk ',...         milk  \n",
       "38      White  [{'transcript': 'that time I thought that I mi...         that  \n",
       "43      White  [{'transcript': 'around and is running around ...         with  \n",
       "47      White  [{'transcript': 'does get a French reversal if...     reversal  \n",
       "48      White  [{'transcript': 'for her fame invented printin...     printing  \n",
       "50      White  [{'transcript': 'he will come if they truly be...   themselves  \n",
       "51      White  [{'transcript': 'like Robert flood who was ess...        flood  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_ten(file_dataframe, \"C:\\\\Users\\\\IBM_ADMIN\\\\speech_recognition\\\\data_output_selected\\\\\", data_version, captcha_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "complete_file_list = glob.glob(\"user_study_output\\\\user_study_initial_output\\\\*.wav\")\n",
    "shuffle(complete_file_list)\n",
    "\n",
    "dbfs_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for file_entry in complete_file_list:\n",
    "    audio = AudioSegment.from_file(file_entry, format=\"wav\")\n",
    "    dbfs_list.append(audio.dBFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-15.930842618\n",
      "4.49456667099\n",
      "-11.436275947\n",
      "169\n"
     ]
    }
   ],
   "source": [
    "mean = np.mean(dbfs_list)\n",
    "std = np.std(dbfs_list)\n",
    "\n",
    "print(mean)\n",
    "print(std)\n",
    "\n",
    "number = 0\n",
    "for dbfs_value in dbfs_list:\n",
    "    number += 1 if dbfs_value > (mean + std) else 0\n",
    "#     if dbfs_value > (mean + std):\n",
    "#         print(dbfs_value)\n",
    "print((mean + std))\n",
    "print(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'uses selected_file_data for creating a stiched audio file'\n",
    "\n",
    "def create_stiched_output(selected_file_data, output_tag):  \n",
    "    output_file_prefix = \"stiched_output\\\\study\" + output_tag\n",
    "\n",
    "    json.dump(selected_file_data, open(output_file_prefix + \".json\", \"w\"))\n",
    "\n",
    "    silence = AudioSegment.silent(duration=1000)\n",
    "    post_number_silence = AudioSegment.silent(duration=500)\n",
    "\n",
    "    source_location = \"reduced_confidence\\\\\"\n",
    "\n",
    "    for tagged_format in [\"_W_B.wav\", \"_O_B.wav\", \".wav\"]:\n",
    "\n",
    "        f = stich_audio_files(selected_file_data, silence, post_number_silence, source_location, tagged_format)\n",
    "\n",
    "        f.export(out_f=\"stiched_output\\\\study\" + output_tag + tagged_format, format=\"wav\")\n",
    "\n",
    "        print(\"Done for {}\", tagged_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Bin the video accordoing to the initial confidence of the strong word.\n",
    "\n",
    "def add_to_appropriate_df(original, variable_set):\n",
    "    for df_dict in variable_set:\n",
    "\n",
    "        first_criteria = (original.first_word_easy == True) & (original.first_confidence > df_dict[\"low\"]) & (original.first_confidence <= df_dict[\"high\"])\n",
    "        second_criteria = (original.first_word_easy == False) & (original.second_confidence > df_dict[\"low\"]) & (original.second_confidence <= df_dict[\"high\"])\n",
    "        \n",
    "        df_dict[\"df\"] = df_dict[\"df\"].append(original[first_criteria])\n",
    "        df_dict[\"df\"] = df_dict[\"df\"].append(original[second_criteria])\n",
    "    \n",
    "    return variable_set\n",
    "\n",
    "\n",
    "variable_set = []\n",
    "\n",
    "step = 5\n",
    "for counter in range(70, 100, step):\n",
    "    variable_set.append({\"low\" : float(counter)/100, \"high\" : float(counter + step)/100, \"df\" : pd.DataFrame()})\n",
    "    \n",
    "variable_set = add_to_appropriate_df(file_dataframe, variable_set)\n",
    "\n",
    "output_tag = \"_Two_Words_\" + str(datetime.now().timestamp()).replace(\".\",\"\")\n",
    "\n",
    "def create_audio_by_confidence():\n",
    "\n",
    "    for df_dict in variable_set:\n",
    "        df = df_dict[\"df\"]\n",
    "        if len(df) == 0:\n",
    "            continue\n",
    "\n",
    "        extended_tag = output_tag + \"_Low_\" + str(df_dict[\"low\"]) + \"_High_\" + str(df_dict[\"high\"])\n",
    "        selected_file_list = list(df.name)[:Number_of_entries]\n",
    "\n",
    "        create_stiched_output(selected_file_list, extended_tag)\n",
    "\n",
    "        print(\"Done for : \", extended_tag)\n",
    "\n",
    "def create_for_complete_list():\n",
    "    # shuffle(complete_file_data)\n",
    "    create_stiched_output(list(file_dataframe.name), output_tag)\n",
    "    \n",
    "create_for_complete_list()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
